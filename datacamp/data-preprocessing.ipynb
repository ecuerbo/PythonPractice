{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing missing data\n",
    "df.drop([1,2,3]) #drops rows by default\n",
    "df.drop('A', axis=1)\n",
    "df.dropna(subset=[\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bddc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "volunteer_cols = volunteer.drop(['Latitude','Longitude'], axis=1)\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols\n",
    "volunteer_subset = volunteer_cols.dropna(subset=['category_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ae303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with data types\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df),\n",
    "                        columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05407d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Subset the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to wine_subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ee8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8b4949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>subscribed</th>\n",
       "      <th>fav_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>y</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>y</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user subscribed fav_color\n",
       "0     1          y      blue\n",
       "1     2          n     green\n",
       "2     3          n    orange\n",
       "3     4          y     green"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#encoding categorical variables\n",
    "user=[1,2,3,4]\n",
    "subscribed = ['y','n','n','y']\n",
    "fav_color = ['blue','green','orange','green']\n",
    "df = pd.DataFrame({'user': user,\"subscribed\":subscribed,'fav_color':fav_color})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c1a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas\n",
    "df['sub_enc'] = df['subscribed'].apply(lambda val:1 if val==\"y\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eeaf767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>subscribed</th>\n",
       "      <th>fav_color</th>\n",
       "      <th>sub_enc</th>\n",
       "      <th>sub_enc_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>y</td>\n",
       "      <td>blue</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>green</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>orange</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>y</td>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user subscribed fav_color  sub_enc  sub_enc_le\n",
       "0     1          y      blue        1           1\n",
       "1     2          n     green        0           0\n",
       "2     3          n    orange        0           0\n",
       "3     4          y     green        1           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding via sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['sub_enc_le'] = le.fit_transform(df['subscribed'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4f1c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>orange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blue  green  orange\n",
       "0     1      0       0\n",
       "1     0      1       0\n",
       "2     0      0       1\n",
       "3     0      1       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "pd.get_dummies(df['fav_color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c869bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56033b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#engineering numerical features\n",
    "temps['mean'] = temps.loc[:,\"day1\":'day3'].mean(axis=1)\n",
    "purchases['date_converted'] = pd.to_datetime(purchases['date'])\n",
    "purchases['month'] = purchases.date_converted.dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbce229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.6\n"
     ]
    }
   ],
   "source": [
    "#engineering text features\n",
    "import re\n",
    "my_string = 'temperature:75.6 F'\n",
    "temp = re.search('\\d+\\.\\d+', my_string)\n",
    "print(float(temp.group(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing text\n",
    "#TF/IDF:Vectorizes words based upon importance(Term Frequency, Inverse Document Frequency)\n",
    "from sklearn.feature_extraction.text import TfidVectorizer\n",
    "tfidf_vec = TfidVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae38233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search('\\d+\\.\\d+', length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking['Length'].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(),y , stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing redundant features\n",
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"locality\", \"region\", \"category_desc\", \"vol_requests\", \"created_date\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting features using text vectors\n",
    "print(tfidf_vec.vocabulary_)\n",
    "\n",
    "print(text_tfidf[3].data)\n",
    "print(text_tfidf[3].indices)\n",
    "\n",
    "#reverse data and indices\n",
    "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b809a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipping together\n",
    "zipped_row = dict(zip(text_tfidf[3].indices,\n",
    "                     text_tfidf[3].data))\n",
    "print(zipped_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef400e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makking a function\n",
    "def return_weights(vocab, vector, vector_index):\n",
    "    \n",
    "    zipped = dict(zip(vector[vector_index].indices,\n",
    "                     vector[vector_index].data))\n",
    "    \n",
    "    return {vocab[i]:zipped[i] for i in vector[vector_index].indices}\n",
    "\n",
    "print(return_weights(vocab,text_tfidf,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(),y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adc3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimentionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df)\n",
    "\n",
    "print(df_pca)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae708713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop('Type', axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit knn to the training data\n",
    "knn.fit(pca_X_train, y_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "print(knn.score(pca_X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame info\n",
    "print(ufo.info())\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype('float')\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a24436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', \"state\", \"type\"]].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna(subset=['length_of_time', \"state\", \"type\"], how='any',axis=0)\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ba5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_minutes(time_string):\n",
    "\n",
    "    # Search for numbers in time_string\n",
    "    num = re.search(\"\\d+\",time_string )\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['length_of_time','minutes']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fefe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds','minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo['seconds_log'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "# Use pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x:1 if x==\"us\" else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo[\"type\"].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head(5))\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date','month','year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedae948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head())\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform desc using vec\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns and rows\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5673ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of features to drop\n",
    "to_drop = ['city', 'country', 'lat', 'long','state', 'date','recorded', 'seconds','minutes','desc','length_of_time']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe88ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `loan_data` subset: loan_data_subset\n",
    "loan_data_subset = loan_data[['Credit Score','Annual Income','Loan Status']]\n",
    "\n",
    "# Create train and test sets\n",
    "trainingSet, testSet = train_test_split(loan_data_subset,test_size=0.2, random_state=123)\n",
    "\n",
    "# Examine pairplots\n",
    "plt.figure()\n",
    "sns.pairplot(trainingSet, hue='Loan Status', palette='RdBu')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.pairplot(testSet, hue='Loan Status', palette='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset loan_data\n",
    "cr_yrs = loan_data['Years of Credit History']\n",
    "\n",
    "# Square root transform\n",
    "cr_yrs_sqrt = boxcox(cr_yrs, lmbda=0.5)\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(cr_yrs_sqrt)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
