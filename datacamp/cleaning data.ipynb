{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a36b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "today_date = dt.date.today()\n",
    "user_signups[user_signups['subscription_date'] > today_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#output movies with rating > 5\n",
    "movies[movies['avg_rating'] > 5]\n",
    "#drop values using filtering\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "#drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace=True)\n",
    "#assert results\n",
    "assert movies['avg_rating'].max <= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5740bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert avg_rating >5 to 5\n",
    "movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to date\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1211286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27,'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e733b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef576f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing a dollar sign from the column content\n",
    "sales['revenue'] = sales['revenue'].str.strip('$')\n",
    "sales['revenue'] = sales['revenue'].astype('int')\n",
    "\n",
    "#verufy theat revenue is now an integer\n",
    "assert sales['revenue'].dtype == 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038d092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1+1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing the weather\n",
    "ri.stop_duration.unique()\n",
    "mapping = {'0-15 Min' : 'short',\n",
    "           '16-30 Min' : 'medium',\n",
    "           '30+ Min' : 'long'}\n",
    "ri['stop_length'] = ri.stop_duration.map(mapping)\n",
    "ri.stop_length.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94026ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.stop_length.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8aeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get duplicates accross all columns\n",
    "duplicates = height_weight.duplicated()\n",
    "\n",
    "#get duplicate  rows\n",
    "height_weight[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find duplicate rows?\n",
    "#subset: list of columns to checjk for duplication\n",
    "#keep: whether to keep first, last or all(False) duplicat values\n",
    "#column names to check for duplication\n",
    "column_names = ['first_name',\"last_name\",\"address\"]\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)\n",
    "height_weight[duplicates].sort_values(by='first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to treat duplicate values?\n",
    "#.drop_duplicated(same args as above)\n",
    "#.groupby and .agg to treat minor differences duplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf68654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by column names and produce statistical summaries\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height':'max', 'weight':'mean'}\n",
    "height_weight = height_weight.groupby(by = column_names).agg(summaries).reset_index()\n",
    "\n",
    "#make sure aggregation is done\n",
    "duplicats = height_weight.duplicated(subset = column_names, keep = False)\n",
    "height_weight[duplicates].sort_values(by='first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c787465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding inconsitent categories\n",
    "inconsitent_cat = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "#dropping inconsistent categories\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_cat)\n",
    "inconsistent_data = study_data[inconsistent_rows]\n",
    "\n",
    "#drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96116fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "marriage_stat['marriage_status'] = mariage_stat[\"marriage_status\"].str.upper()\n",
    "marriage_stat[\"marriage_status\"].value_counts()\n",
    "\n",
    "#strip all spaces\n",
    "demo = demo['marriage_status'].str.strip()\n",
    "demo['marriage_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c959259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapsing data into categories\n",
    "#using qcut()\n",
    "group_names = ['0-200K', \"200-500K\",'500K+']\n",
    "demo['income_group'] = pd.qcut(demo['household_income'], q = 3,labels=group_names)\n",
    "#print income_group column\n",
    "demo[[\"income_group\",\"household_income\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a22e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cut()\n",
    "ranges = [0,200000,500000,np.inf]\n",
    "group_names = ['0-200K', \"200-500K\",'500K+']\n",
    "#create income_group columns\n",
    "demo['income_group'] = pd.cut(demo['household_income'], bins=ranges,labels=group_names)\n",
    "demo[[\"income_group\",\"household_income\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267de6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map categories to fewer ones\n",
    "mapping = {\"Microsoft\":'DesktopOS',\"MacOS\":'DesktopOS',\"Linuz\":'DesktopOS','IOS':\"MobileOS\",'IOS':\"MobileOS\"}\n",
    "devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
    "devices['operating_system'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00295586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', \"medium\", \"long\"]\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5643dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phones[\"phon_num\"] = phones[\"phon_num\"].str.replace(\"+\",\"00\")\n",
    "phones[\"phon_num\"] = phones[\"phon_num\"].str.replace(\"-\",\"\")\n",
    "\n",
    "#replace phone numbers with lower than 10 digits to NaN\n",
    "digits = phones[\"phon_num\"].str.len()\n",
    "phones.loc[digits<10,\"phone_num\"] == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check = phones['phone_num'].str.len()\n",
    "assert sanity_check.min() >=10\n",
    "assert phones[\"phone_num\"].str.contains(\"+|-\").any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e993a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey[\"survey_response\"].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6867fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing datatype fro object to category\n",
    "cats = pd.CategoricalDtype(['short','medium','long'], ordered=True)\n",
    "ri['stop_length'] = ri.stop_length.astype(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ordered categories\n",
    "ri[ri.stp_length > 'short'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy 'WT01' through 'WT22' to a new DataFrame\n",
    "WT = weather.loc[:,'WT01':'WT22']\n",
    "\n",
    "# Calculate the sum of each row in 'WT'\n",
    "weather['bad_conditions'] = WT.sum(axis='columns')\n",
    "\n",
    "# Replace missing values in 'bad_conditions' with '0'\n",
    "weather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n",
    "\n",
    "# Create a histogram to visualize 'bad_conditions'\n",
    "weather['bad_conditions'].plot(kind='hist')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in 'bad_conditions' and sort the index\n",
    "print(weather.bad_conditions.value_counts().sort_index())\n",
    "\n",
    "# Create a dictionary that maps integers to strings\n",
    "mapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad',5:'worse',6:'worse',7:'worse',8:'worse',9:'worse'}\n",
    "\n",
    "# Convert the 'bad_conditions' integers to strings using the 'mapping'\n",
    "weather['rating'] = weather.bad_conditions.map(mapping)\n",
    "\n",
    "# Count the unique values in 'rating'\n",
    "print(weather['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db603560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the logical order of the weather ratings\n",
    "cats = pd.CategoricalDtype(['good', 'bad','worse'], ordered=True)\n",
    "\n",
    "# Change the data type of 'rating' to category\n",
    "weather['rating'] = weather.rating.astype(cats)\n",
    "\n",
    "# Examine the head of 'rating'\n",
    "print(weather['rating'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33759fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_fah = temperatures.loc[temperatures['Temperature'] > 40, 'Temperature']\n",
    "temp_cels = (temp_fah-32)*(5/9)\n",
    "temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating date data\n",
    "#converts to date time but wont work if weird formats occur!\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'])\n",
    "\n",
    "#will !\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'],\n",
    "                                      #attempt to infer formst of each date\n",
    "                                      infer_date_format=True,\n",
    "                                      #Return NA for rows where conversion failed\n",
    "                                      errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to date time format of your choice\n",
    "birthdays['Birthday'] = birthdays['Birthday'].dt.strftime(\"%d-%m-%Y\")\n",
    "birthdays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aca642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3639f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare reeding versus reading\n",
    "fuzz.WRatio('Reeding','Reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac33eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rockets vs Lakers', 86, 0), ('Lakers vs Rockets', 86, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comparison with arrays\n",
    "from thefuzz import process\n",
    "\n",
    "#define the string and array of possible matches\n",
    "string = 'Houston Rockets vs Los Angeles Lakers'\n",
    "choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets', 'Houston vs Los Angeles','Heat vs Bulls'])\n",
    "\n",
    "process.extract(string, choices, limit = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda8bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(survey['state'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapsing all the state\n",
    "for state in categories['state']:\n",
    "    #find potential matches in states with typoes\n",
    "    matches = process.extract(state, survey['state'], limit = survey.shape[0])\n",
    "    #for each potential match match\n",
    "    for potential_match in matches:\n",
    "        #if high similarity score\n",
    "        if potential_match[1] >= 80:\n",
    "            #replace typo with correct category\n",
    "            survey.loc[survey['state'] == potential_match[0], 'state'] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "\n",
    "#create indexing object\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "#generate pair blocked on state\n",
    "indexer.block('state')\n",
    "pairs = indexer.index(census_A,census_B)\n",
    "\n",
    "#generate a compare object\n",
    "compare_cl = recordlinkage.Compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2582ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find exact matches for pairs of date_of_birth and state\n",
    "compare_cl.exact('date_of_birth','date_of_birth',label='date_of_birth')\n",
    "compare_cl.exact('state','state',label='state')\n",
    "\n",
    "# Find similar matches for pairs of surname and address_1 using string similarity\n",
    "compare_cl.string('surname', 'surname', threshold=0.85,label='surname')\n",
    "compare_cl.string('address_1', 'address_1', threshold=0.85,label='address_1')\n",
    "\n",
    "find matches\n",
    "potential_matches = compare_cl.compute(pairs, census_A, census_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types - \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)\n",
    "\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "matches.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get indices  from census_B only\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "print(census_B_index)\n",
    "\n",
    "#finding duplicates in census_B\n",
    "census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n",
    "\n",
    "#finding new rows in census_B\n",
    "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "\n",
    "#link the DataFrames!\n",
    "full_census = census_A.append(census_B_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a8b84",
   "metadata": {},
   "source": [
    "### UNIFORMITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e128a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=\"Date\", y='Temperature', data= temperatures)\n",
    "plt.title('Temperature in Celsius March 2019-NYC')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Temperature in Celsius')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate outlying data\n",
    "temp_fah = temp.loc[temp['Temperature']>40,'Temperature']\n",
    "temp_cels = (temp_fah-32)*(5/9)\n",
    "temp.loc[temp['Temperature']>40,'Temperature'] = temp_cels\n",
    "\n",
    "#assert conversion is correct\n",
    "assert temp['Temperature'].max() < 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f552092",
   "metadata": {},
   "source": [
    "## CROSS FIELD VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b15d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_classes = flights[['economy_class','business_class', 'first_class']].sum(axis = 1)\n",
    "passenger_equ = sum_classes == flights['total_passengers']\n",
    "#find and filter out rows with inconsistent passenger totals\n",
    "inconsistent_pass = flights[~passenger_equ]\n",
    "consistent_pass = flights[passenger_equ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "users['Birthday'] = pd.to_datetime(users['Birthday'])\n",
    "today = dt.date.today()\n",
    "age_manual = today.year - users['Birthday'].dt.year\n",
    "\n",
    "#find instances where ages match\n",
    "age_equ = age_manual == users['Age']\n",
    "\n",
    "inconsistent_age = users[~age_equ]\n",
    "consistent_age = users[age_equ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44274b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "#visualize missingness\\\n",
    "msno.matrix(airquality)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate missing and complete values aside\n",
    "missing = airquality[airquality['CO2'].isna()]\n",
    "complete = airquality[~airquality['CO2'].isna()]\n",
    "\n",
    "#sorted airquality\n",
    "sorted_airq = airquality.sort_values(by='Tempartaure')\n",
    "msno.matrix(sorted_airq);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing values\n",
    "airq_dropped = airquality.dropna(subset=['CO2'])\n",
    "\n",
    "#replacing with statistical measures\n",
    "co2_mean = airquality['CO2'].mean()\n",
    "airq_imputed = airquality.fillna({'CO2':co2_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "diabetes = pd.read_csv('pima-indians-diaabetes-data.csv')\n",
    "msno.heatmap(diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5303be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missingness dendrogram\n",
    "msno.dendrogram(diabetes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
